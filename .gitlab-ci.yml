# This file is a template, and might need editing before it works on your project.
# This is a sample GitLab CI/CD configuration file that should run without any modifications.
# It demonstrates a basic 3 stage CI/CD pipeline. Instead of real tests or scripts,
# it uses echo commands to simulate the pipeline execution.
#
# A pipeline is composed of independent jobs that run scripts, grouped into stages.
# Stages run in sequential order, but jobs within stages run in parallel.
#
# For more information, see: https://docs.gitlab.com/ee/ci/yaml/#stages
#
# You can copy and paste this template into a new `.gitlab-ci.yml` file.
# You should not add this template to an existing `.gitlab-ci.yml` file by using the `include:` keyword.
#
# To contribute improvements to CI/CD templates, please follow the Development guide at:
# https://docs.gitlab.com/ee/development/cicd/templates.html
# This specific template is located at:
# https://gitlab.com/gitlab-org/gitlab/-/blob/master/lib/gitlab/ci/templates/Getting-Started.gitlab-ci.yml



stages:
  - build
  - test
  - deploy

variables:
  PYTHON_VERSION: "3.13"
  UV_VERSION: "0.5.5"
  BASE_LAYER: bookworm-slim
  IMAGE: ghcr.io/astral-sh/uv:$UV_VERSION-python$PYTHON_VERSION-$BASE_LAYER

image: $IMAGE

before_script:
  - echo "Setting up the environment..."
  - uv run python3 --version


###########################################
#                                         #
#               Build Stage               #
#                                         #
###########################################

uv-install:
  stage: build
  variables:
    UV_CACHE_DIR: .uv-cache
  cache:
    key:
      files:
        - uv.lock
    paths:
      - $UV_CACHE_DIR
  script:
    - uv cache prune --ci
    - uv pip install --system -e .


###########################################
#                                         #
#               Test Stage                #
#                                         #
###########################################

# This job runs the tests using Pytest
pytest:
  stage: test
  script:
    - echo "Running Pytest..."
    - mkdir $HOME/mosek
    - touch $HOME/mosek/mosek.lic
    - echo "$MOSEK_LICENSE" >> $HOME/mosek/mosek.lic
    - echo "Mosek license file created at $HOME/mosek/mosek.lic\n"
    # Executes pytest to run all test files in the project.
    # Options:
    # - `--durations=10`: Shows the 10 slowest test executions
    # - `--cov=src`: Measures code coverage for the 'src' directory
    # - `--cov-report=xml:docs/tools-source/coverage.xml`: Generates an XML coverage report at the specified path
    - uv run pytest --durations=10 --cov=src --cov-report=xml:docs/tools-source/coverage.xml
  coverage: '/TOTAL.*\s+(\d+%)$/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: docs/tools-source/coverage.xml
    paths:
      - docs/tools-source/coverage.xml
    expire_in: 1 week

# This job checks the code for type errors using Mypy
mypy:
  stage: test
  script:
    - echo "Running Mypy..."
    - uv run mypy
    - echo "Mypy complete"
  allow_failure: false

# This job checks the code for style violations using Ruff
ruff-check-src:
  stage: test
  script:
    - echo "Running Ruff..."
    - uvx ruff check src
    - echo "Ruff complete"
  allow_failure: true

# ruff-check-tests:
#   stage: test
#   script:
#     - echo "Running Ruff..."
#     - uvx ruff check tests
#     - echo "Ruff complete"
#   allow_failure: true

# # Add a new job for code quality metrics
# code-quality:
#   stage: test
#   image: docker:stable
#   variables:
#     DOCKER_DRIVER: overlay2
#   allow_failure: true
#   services:
#     - docker:dind
#   # Skip the global before_script that requires uv
#   before_script: []
#   script:
#     - docker run --env CODECLIMATE_CODE="$PWD" --volume "$PWD":/code --volume /var/run/docker.sock:/var/run/docker.sock pipelinecomponents/codeclimate analyze -f json > gl-code-quality-report.json || echo "CodeClimate analysis failed"
#     - if [ -f "gl-code-quality-report.json" ]; then echo "Report file generated successfully"; else echo "Report file not found" && touch gl-code-quality-report.json && echo "[]" > gl-code-quality-report.json; fi
#     - ls -la gl-code-quality-report.json
#     - pwd
#     # Make sure the file has the right permissions and is in the expected location
#     - chmod 644 gl-code-quality-report.json
#   artifacts:
#     paths:
#       - gl-code-quality-report.json
#     reports:
#       codequality: gl-code-quality-report.json
#     expire_in: 2 week

# # Add a new job for performance metrics
# performance:
#   stage: test
#   script:
#     - uv pip install --system pytest-benchmark pytest-benchmark-k pandas matplotlib
#     - uv run pytest testing/performance --benchmark-json=benchmark.json
#     # Generate HTML report
#     - uv run python -c "
#         import json;
#         from pytest_benchmark_k import render;
#         with open('benchmark.json') as f:
#             data = json.load(f);
#         render.render_html(data, 'benchmark_report.html')"
#     # Create directory structure for benchmarks if it doesn't exist
#     - mkdir -p testing/benchmarks
#     # Track performance history if running on main branch
#     - |
#       if [[ "$CI_COMMIT_BRANCH" == "main" ]]; then
#         echo "Updating performance history database..."
#         uv run python testing/benchmarks/track_performance.py benchmark.json testing/benchmarks/benchmark_history.db testing/benchmarks/charts
#         mkdir -p public/performance
#         cp -r testing/benchmarks/charts/* public/performance/ || echo "No charts yet"
#       fi
#   artifacts:
#     paths:
#       - benchmark.json
#       - benchmark_report.html
#       - testing/benchmarks/benchmark_history.db
#       - testing/benchmarks/charts
#     reports:
#       performance: benchmark.json
#     expire_in: 2 week
#   # only:
#   #   - main  # Only run on main branch to avoid noise

###########################################
#                                         #
#               Deploy Stage              #
#                                         #
###########################################

pages:
  stage: deploy
  script:
  - echo "Deploying documentation..."
  - uv run sphinx-build --version
  - uv run sphinx-build -b html docs/source public

  # - echo "Copying performance reports..."
  # - mkdir -p public/performance
  # - if [ -f "benchmark_report.html" ]; then cp benchmark_report.html public/performance/; fi
  # - if [ -f "benchmark.json" ]; then cp benchmark.json public/performance/; fi
  artifacts:
    paths:
    - public
